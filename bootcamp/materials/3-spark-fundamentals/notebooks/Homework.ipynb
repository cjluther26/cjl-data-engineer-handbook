{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "777bbd6d-45ac-4c8e-af47-d6549cf31f3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/01/28 20:20:27 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://9be1677be336:4041\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PySparkShell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0xffff93f2ea00>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import expr, col, broadcast, to_timestamp\n",
    "\n",
    "# Build spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Jupyter\") \\\n",
    "    .config(\"spark.driver.memory\", \"15g\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Deactivate automatic broadcast joins\n",
    "spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", \"-1\")\n",
    "\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2edd4c81-7fba-43d3-abfb-fbbba57f49ce",
   "metadata": {},
   "source": [
    "## Data\n",
    "**match_details**\n",
    "- a row for every players performance in a match\n",
    "  \n",
    "**matches**\n",
    "- a row for every match\n",
    "\n",
    "**medals_matches_players**\n",
    "- a row for every medal type a player gets in a match\n",
    "     \n",
    "**medals**\n",
    "- a row for every medal type"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a94b9e9f-fe40-4add-824c-9aea48a324d8",
   "metadata": {},
   "source": [
    "## Homework Assignment\n",
    "\n",
    "1. Build a Spark job that\n",
    "- Disables automatic broadcast join with `spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", \"-1\")`\n",
    "- Explicitly broadcast JOINs `matches` and `maps`\n",
    "- Bucket join `match_details`, `matches`, and `medals_matches_players` on `match_id` with 16 buckets\n",
    "\n",
    "2. Aggregate the joined data frame to figure out questions like:\n",
    "- Which player averages the most kills per game?\n",
    "- Which playlist gets played the most?\n",
    "- Which map gets played the most?\n",
    "- Which map do players get the most Killing Spree medals on?\n",
    "\n",
    "3. Use the aggregated data set to:\n",
    "- Try different `.sortWithinPartitions` to see which has the smallest data size (hint: `playlists` and `maps` are both very low cardinality)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "42144e36-0ff6-4da8-9398-1f65f0dbc1b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(match_id='11de1a94-8d07-4162-9f5f-d3cc753c811c', mapid='c7edbf0f-f206-11e4-aa52-24be05e24f7e', is_team_game='true', playlist_id='f72e0ef0-7c4a-4307-af78-8e38dac3fdba', game_variant_id='1e473914-46e4-408d-af26-178fb115de76', is_match_over='true', completion_date='2016-02-22 00:00:00.000000', match_duration=None, game_mode=None, map_variant_id=None, event_ts=datetime.datetime(2016, 2, 22, 0, 0), event_date=datetime.datetime(2016, 2, 22, 0, 0)),\n",
       " Row(match_id='d3643e71-3e51-43e6-a200-f4a7f306ac12', mapid='cb914b9e-f206-11e4-b447-24be05e24f7e', is_team_game='false', playlist_id='d0766624-dbd7-4536-ba39-2d890a6143a9', game_variant_id='257a305e-4dd3-41f1-9824-dfe7e8bd59e1', is_match_over='true', completion_date='2016-02-14 00:00:00.000000', match_duration=None, game_mode=None, map_variant_id=None, event_ts=datetime.datetime(2016, 2, 14, 0, 0), event_date=datetime.datetime(2016, 2, 14, 0, 0)),\n",
       " Row(match_id='d78d2aae-36e4-48ac-a3b5-6d4d90f90ace', mapid='c7edbf0f-f206-11e4-aa52-24be05e24f7e', is_team_game='true', playlist_id='f72e0ef0-7c4a-4307-af78-8e38dac3fdba', game_variant_id='1e473914-46e4-408d-af26-178fb115de76', is_match_over='true', completion_date='2016-03-24 00:00:00.000000', match_duration=None, game_mode=None, map_variant_id='55e5ee2e-88df-4657-b9ae-b6ec7ca64614', event_ts=datetime.datetime(2016, 3, 24, 0, 0), event_date=datetime.datetime(2016, 3, 24, 0, 0)),\n",
       " Row(match_id='b440069e-ec5f-4f51-bdd1-bc0bc7fe1195', mapid='c7edbf0f-f206-11e4-aa52-24be05e24f7e', is_team_game='true', playlist_id='f72e0ef0-7c4a-4307-af78-8e38dac3fdba', game_variant_id='1e473914-46e4-408d-af26-178fb115de76', is_match_over='true', completion_date='2015-12-23 00:00:00.000000', match_duration=None, game_mode=None, map_variant_id='ec3eef73-13e3-4d4b-a922-cc195109a842', event_ts=datetime.datetime(2015, 12, 23, 0, 0), event_date=datetime.datetime(2015, 12, 23, 0, 0)),\n",
       " Row(match_id='1dd475fc-ee6b-4e1d-8140-c44d03812076', mapid='c93d708f-f206-11e4-a815-24be05e24f7e', is_team_game='true', playlist_id='0e39ead4-383b-4452-bbd4-babb7becd82e', game_variant_id='42f97cca-2cb4-497a-a0fd-ceef1ba46bcc', is_match_over='true', completion_date='2016-04-07 00:00:00.000000', match_duration=None, game_mode=None, map_variant_id=None, event_ts=datetime.datetime(2016, 4, 7, 0, 0), event_date=datetime.datetime(2016, 4, 7, 0, 0))]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load data\n",
    "medals = spark.read.option(\"header\", \"true\") \\\n",
    "                .csv(\"/home/iceberg/data/medals.csv\")\n",
    "\n",
    "maps = spark.read.option(\"header\", \"true\") \\\n",
    "                .csv(\"/home/iceberg/data/maps.csv\")\n",
    "\n",
    "match_details = spark.read.option(\"header\", \"true\") \\\n",
    "                .csv(\"/home/iceberg/data/match_details.csv\")\n",
    "\n",
    "matches = spark.read.option(\"header\", \"true\") \\\n",
    "                .csv(\"/home/iceberg/data/matches.csv\") \\\n",
    "                .withColumn(\"event_ts\", to_timestamp(\"completion_date\")) \\\n",
    "                .withColumn(\"event_date\", expr(\"DATE_TRUNC('DAY', completion_date)\"))\n",
    "\n",
    "medals_matches_players = spark.read.option(\"header\", \"true\") \\\n",
    "                .csv(\"/home/iceberg/data/medals_matches_players.csv\")\n",
    "\n",
    "matches.take(5)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "28c024cc-d921-4c72-8b8a-0217ea230ee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Broadcast JOIN `matches` and `maps`\n",
    "\n",
    "matches_df = matches.join(\n",
    "    broadcast(maps),\n",
    "    matches[\"mapid\"] == maps[\"mapid\"],\n",
    "    how = \"inner\"\n",
    ")#.explain()\n",
    "\n",
    "# matches_df.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d715c670-1e42-49a5-9b79-a1f6f0f56ea4",
   "metadata": {},
   "source": [
    "### Bucket join `match_details`, `matches`, and `medals_matches_players` on `match_id` with 16 buckets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0bd78d2f-3295-4887-98d9-893104c8d73d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- match_id: string (nullable = true)\n",
      " |-- player_gamertag: string (nullable = true)\n",
      " |-- medal_id: string (nullable = true)\n",
      " |-- count: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "medals_matches_players.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd5d8805-f54a-4545-a344-426bc4644b7c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0e202c22-294c-4ed9-9519-4ef2ab55c188",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "    <thead>\n",
       "        <tr>\n",
       "        </tr>\n",
       "    </thead>\n",
       "    <tbody>\n",
       "    </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "++\n",
       "||\n",
       "++\n",
       "++"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%sql\n",
    "CREATE DATABASE IF NOT EXISTS bootcamp_hw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a95b39fe-c493-481a-ae76-d1cbc2da9880",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "605c3ac2-bdcc-48b3-b9fd-e161d7415ff4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "    <thead>\n",
       "        <tr>\n",
       "        </tr>\n",
       "    </thead>\n",
       "    <tbody>\n",
       "    </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "++\n",
       "||\n",
       "++\n",
       "++"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%sql\n",
    "DROP TABLE IF EXISTS bootcamp_hw.match_details_bucketed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5e34aac4-e6f0-469b-aabd-374e70acb7df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "    <thead>\n",
       "        <tr>\n",
       "        </tr>\n",
       "    </thead>\n",
       "    <tbody>\n",
       "    </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "++\n",
       "||\n",
       "++\n",
       "++"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%sql\n",
    "DROP TABLE IF EXISTS bootcamp_hw.matches_bucketed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2622c054-f072-4977-9cea-08cffb3404b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "    <thead>\n",
       "        <tr>\n",
       "        </tr>\n",
       "    </thead>\n",
       "    <tbody>\n",
       "    </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "++\n",
       "||\n",
       "++\n",
       "++"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%sql\n",
    "DROP TABLE IF EXISTS bootcamp_hw.medals_matches_players_bucketed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "93afc8ed-beb1-41ed-8d32-b40b525c0bdd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "    <thead>\n",
       "        <tr>\n",
       "        </tr>\n",
       "    </thead>\n",
       "    <tbody>\n",
       "    </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "++\n",
       "||\n",
       "++\n",
       "++"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%sql\n",
    "DROP TABLE IF EXISTS bootcamp_hw.matches_full_details_bucketed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "de67d239-7375-4bcd-ad3c-376447105554",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/01/28 20:20:38 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "    <thead>\n",
       "        <tr>\n",
       "        </tr>\n",
       "    </thead>\n",
       "    <tbody>\n",
       "    </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "++\n",
       "||\n",
       "++\n",
       "++"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%sql\n",
    "\n",
    "CREATE TABLE IF NOT EXISTS bootcamp_hw.match_details_bucketed(\n",
    "    match_id STRING,\n",
    "    player_gamertag STRING,\n",
    "    previous_spartan_rank STRING,\n",
    "    spartan_rank STRING,\n",
    "    previous_total_xp STRING,\n",
    "    total_xp STRING,\n",
    "    previous_csr_tier STRING,\n",
    "    previous_csr_designation STRING,\n",
    "    previous_csr STRING,\n",
    "    previous_csr_percent_to_next_tier STRING,\n",
    "    previous_csr_rank STRING,\n",
    "    current_csr_tier STRING,\n",
    "    current_csr_designation STRING,\n",
    "    current_csr STRING,\n",
    "    current_csr_percent_to_next_tier STRING,\n",
    "    current_csr_rank STRING,\n",
    "    player_rank_on_team STRING,\n",
    "    player_finished STRING,\n",
    "    player_average_life STRING,\n",
    "    player_total_kills STRING,\n",
    "    player_total_headshots STRING,\n",
    "    player_total_weapon_damage STRING,\n",
    "    player_total_shots_landed STRING,\n",
    "    player_total_melee_kills STRING,\n",
    "    player_total_melee_damage STRING,\n",
    "    player_total_assassinations STRING,\n",
    "    player_total_ground_pound_kills STRING,\n",
    "    player_total_shoulder_bash_kills STRING,\n",
    "    player_total_grenade_damage STRING,\n",
    "    player_total_power_weapon_damage STRING,\n",
    "    player_total_power_weapon_grabs STRING,\n",
    "    player_total_deaths STRING,\n",
    "    player_total_assists STRING,\n",
    "    player_total_grenade_kills STRING,\n",
    "    did_win STRING,\n",
    "    team_id STRING\n",
    ")\n",
    "USING parquet\n",
    "PARTITIONED BY (bucket(16, match_id));\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e0672a00-909c-449c-8eab-dc4b2eae51d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "    <thead>\n",
       "        <tr>\n",
       "        </tr>\n",
       "    </thead>\n",
       "    <tbody>\n",
       "    </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "++\n",
       "||\n",
       "++\n",
       "++"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%sql\n",
    "\n",
    "CREATE TABLE IF NOT EXISTS bootcamp_hw.matches_bucketed(\n",
    "    match_id STRING,\n",
    "    mapid STRING,\n",
    "    is_team_game STRING,\n",
    "    playlist_id STRING,\n",
    "    game_variant_id STRING,\n",
    "    is_match_over STRING,\n",
    "    completion_date TIMESTAMP,\n",
    "    match_duration STRING,\n",
    "    game_mode STRING,\n",
    "    map_variant_id STRING,\n",
    "    event_date DATE\n",
    ")\n",
    "USING parquet\n",
    "PARTITIONED BY (bucket(16, match_id));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cc385532-94be-4159-aeee-1565d96aa83f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "    <thead>\n",
       "        <tr>\n",
       "        </tr>\n",
       "    </thead>\n",
       "    <tbody>\n",
       "    </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "++\n",
       "||\n",
       "++\n",
       "++"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%sql\n",
    "\n",
    "CREATE TABLE IF NOT EXISTS bootcamp_hw.medals_matches_players_bucketed(\n",
    "    match_id STRING,\n",
    "    player_gamertag STRING,\n",
    "    medal_id STRING,\n",
    "    count STRING\n",
    ")\n",
    "USING parquet\n",
    "PARTITIONED BY (bucket(16, match_id));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a50bfbfb-f714-40bf-abcb-0f7e22c260ef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7685b74c-8f23-4a8e-887d-c1403ff5ebd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/01/28 20:20:46 ERROR Utils: Aborting task                      (0 + 11) / 11]\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "\tat java.base/java.io.ByteArrayOutputStream.<init>(ByteArrayOutputStream.java:79)\n",
      "\tat org.apache.iceberg.shaded.org.apache.parquet.hadoop.CodecFactory$HeapBytesCompressor.<init>(CodecFactory.java:158)\n",
      "\tat org.apache.iceberg.shaded.org.apache.parquet.hadoop.CodecFactory.createCompressor(CodecFactory.java:219)\n",
      "\tat org.apache.iceberg.shaded.org.apache.parquet.hadoop.CodecFactory.getCompressor(CodecFactory.java:202)\n",
      "\tat org.apache.iceberg.parquet.ParquetWriter.<init>(ParquetWriter.java:90)\n",
      "\tat org.apache.iceberg.parquet.Parquet$WriteBuilder.build(Parquet.java:352)\n",
      "\tat org.apache.iceberg.parquet.Parquet$DataWriteBuilder.build(Parquet.java:734)\n",
      "\tat org.apache.iceberg.data.BaseFileWriterFactory.newDataWriter(BaseFileWriterFactory.java:131)\n",
      "\tat org.apache.iceberg.io.RollingDataWriter.newWriter(RollingDataWriter.java:52)\n",
      "\tat org.apache.iceberg.io.RollingDataWriter.newWriter(RollingDataWriter.java:32)\n",
      "\tat org.apache.iceberg.io.RollingFileWriter.openCurrentWriter(RollingFileWriter.java:108)\n",
      "\tat org.apache.iceberg.io.RollingDataWriter.<init>(RollingDataWriter.java:47)\n",
      "\tat org.apache.iceberg.io.FanoutDataWriter.newWriter(FanoutDataWriter.java:53)\n",
      "\tat org.apache.iceberg.io.FanoutWriter.writer(FanoutWriter.java:63)\n",
      "\tat org.apache.iceberg.io.FanoutWriter.write(FanoutWriter.java:51)\n",
      "\tat org.apache.iceberg.io.FanoutDataWriter.write(FanoutDataWriter.java:31)\n",
      "\tat org.apache.iceberg.spark.source.SparkWrite$PartitionedDataWriter.write(SparkWrite.java:781)\n",
      "\tat org.apache.iceberg.spark.source.SparkWrite$PartitionedDataWriter.write(SparkWrite.java:751)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.write(WriteToDataSourceV2Exec.scala:493)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.$anonfun$run$1(WriteToDataSourceV2Exec.scala:448)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask$$Lambda$4150/0x0000000101813840.apply(Unknown Source)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1397)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run(WriteToDataSourceV2Exec.scala:486)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run$(WriteToDataSourceV2Exec.scala:425)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.run(WriteToDataSourceV2Exec.scala:491)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.$anonfun$writeWithV2$2(WriteToDataSourceV2Exec.scala:388)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec$$Lambda$3932/0x000000010178d840.apply(Unknown Source)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$Lambda$2863/0x00000001013e6840.apply(Unknown Source)\n",
      "25/01/28 20:20:46 ERROR Utils: Aborting task\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "\tat java.base/java.io.ByteArrayOutputStream.<init>(ByteArrayOutputStream.java:79)\n",
      "\tat org.apache.iceberg.shaded.org.apache.parquet.hadoop.CodecFactory$HeapBytesCompressor.<init>(CodecFactory.java:158)\n",
      "\tat org.apache.iceberg.shaded.org.apache.parquet.hadoop.CodecFactory.createCompressor(CodecFactory.java:219)\n",
      "\tat org.apache.iceberg.shaded.org.apache.parquet.hadoop.CodecFactory.getCompressor(CodecFactory.java:202)\n",
      "\tat org.apache.iceberg.parquet.ParquetWriter.<init>(ParquetWriter.java:90)\n",
      "\tat org.apache.iceberg.parquet.Parquet$WriteBuilder.build(Parquet.java:352)\n",
      "\tat org.apache.iceberg.parquet.Parquet$DataWriteBuilder.build(Parquet.java:734)\n",
      "\tat org.apache.iceberg.data.BaseFileWriterFactory.newDataWriter(BaseFileWriterFactory.java:131)\n",
      "\tat org.apache.iceberg.io.RollingDataWriter.newWriter(RollingDataWriter.java:52)\n",
      "\tat org.apache.iceberg.io.RollingDataWriter.newWriter(RollingDataWriter.java:32)\n",
      "\tat org.apache.iceberg.io.RollingFileWriter.openCurrentWriter(RollingFileWriter.java:108)\n",
      "\tat org.apache.iceberg.io.RollingDataWriter.<init>(RollingDataWriter.java:47)\n",
      "\tat org.apache.iceberg.io.FanoutDataWriter.newWriter(FanoutDataWriter.java:53)\n",
      "\tat org.apache.iceberg.io.FanoutWriter.writer(FanoutWriter.java:63)\n",
      "\tat org.apache.iceberg.io.FanoutWriter.write(FanoutWriter.java:51)\n",
      "\tat org.apache.iceberg.io.FanoutDataWriter.write(FanoutDataWriter.java:31)\n",
      "\tat org.apache.iceberg.spark.source.SparkWrite$PartitionedDataWriter.write(SparkWrite.java:781)\n",
      "\tat org.apache.iceberg.spark.source.SparkWrite$PartitionedDataWriter.write(SparkWrite.java:751)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.write(WriteToDataSourceV2Exec.scala:493)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.$anonfun$run$1(WriteToDataSourceV2Exec.scala:448)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask$$Lambda$4150/0x0000000101813840.apply(Unknown Source)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1397)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run(WriteToDataSourceV2Exec.scala:486)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run$(WriteToDataSourceV2Exec.scala:425)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.run(WriteToDataSourceV2Exec.scala:491)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.$anonfun$writeWithV2$2(WriteToDataSourceV2Exec.scala:388)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec$$Lambda$3932/0x000000010178d840.apply(Unknown Source)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$Lambda$2863/0x00000001013e6840.apply(Unknown Source)\n",
      "25/01/28 20:20:46 ERROR DataWritingSparkTask: Aborting commit for partition 9 (task 22, attempt 0, stage 8.0)\n",
      "25/01/28 20:20:46 ERROR Utils: Aborting task\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "25/01/28 20:20:46 ERROR DataWritingSparkTask: Aborting commit for partition 3 (task 16, attempt 0, stage 8.0)\n",
      "25/01/28 20:20:46 ERROR DataWritingSparkTask: Aborting commit for partition 0 (task 13, attempt 0, stage 8.0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[22.652s][warning][gc,alloc] Executor task launch worker for task 5.0 in stage 8.0 (TID 18): Retried waiting for GCLocker too often allocating 131074 words\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/01/28 20:20:46 ERROR Utils: Aborting task\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "25/01/28 20:20:46 ERROR DataWritingSparkTask: Aborting commit for partition 5 (task 18, attempt 0, stage 8.0)\n",
      "25/01/28 20:20:46 ERROR Utils: Aborting task\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "25/01/28 20:20:46 ERROR DataWritingSparkTask: Aborting commit for partition 2 (task 15, attempt 0, stage 8.0)\n",
      "25/01/28 20:20:46 ERROR Utils: Aborting task\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "25/01/28 20:20:46 ERROR DataWritingSparkTask: Aborting commit for partition 10 (task 23, attempt 0, stage 8.0)\n",
      "25/01/28 20:20:47 ERROR Utils: Aborting task\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "25/01/28 20:20:47 ERROR DataWritingSparkTask: Aborting commit for partition 4 (task 17, attempt 0, stage 8.0)\n",
      "25/01/28 20:20:47 ERROR Utils: Aborting task\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "25/01/28 20:20:47 ERROR Utils: Aborting task\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "25/01/28 20:20:47 ERROR DataWritingSparkTask: Aborting commit for partition 1 (task 14, attempt 0, stage 8.0)\n",
      "25/01/28 20:20:47 ERROR DataWritingSparkTask: Aborting commit for partition 8 (task 21, attempt 0, stage 8.0)\n",
      "25/01/28 20:20:47 ERROR Utils: Aborting task\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "25/01/28 20:20:47 ERROR DataWritingSparkTask: Aborting commit for partition 7 (task 20, attempt 0, stage 8.0)\n",
      "25/01/28 20:20:47 ERROR Utils: Aborting task\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "25/01/28 20:20:47 ERROR DataWritingSparkTask: Aborting commit for partition 6 (task 19, attempt 0, stage 8.0)\n",
      "25/01/28 20:20:51 ERROR DataWritingSparkTask: Aborted commit for partition 0 (task 13, attempt 0, stage 8.0)\n",
      "25/01/28 20:20:51 ERROR DataWritingSparkTask: Aborted commit for partition 5 (task 18, attempt 0, stage 8.0)\n",
      "25/01/28 20:20:51 ERROR DataWritingSparkTask: Aborted commit for partition 7 (task 20, attempt 0, stage 8.0)\n",
      "25/01/28 20:20:51 ERROR DataWritingSparkTask: Aborted commit for partition 10 (task 23, attempt 0, stage 8.0)\n",
      "25/01/28 20:20:51 ERROR DataWritingSparkTask: Aborted commit for partition 2 (task 15, attempt 0, stage 8.0)\n",
      "25/01/28 20:20:51 ERROR DataWritingSparkTask: Aborted commit for partition 3 (task 16, attempt 0, stage 8.0)\n",
      "25/01/28 20:20:51 ERROR Executor: Exception in task 2.0 in stage 8.0 (TID 15)\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "25/01/28 20:20:51 ERROR Executor: Exception in task 7.0 in stage 8.0 (TID 20)\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "25/01/28 20:20:51 ERROR Executor: Exception in task 10.0 in stage 8.0 (TID 23)\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "25/01/28 20:20:51 ERROR Executor: Exception in task 5.0 in stage 8.0 (TID 18)\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "25/01/28 20:20:51 ERROR Executor: Exception in task 0.0 in stage 8.0 (TID 13)\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "25/01/28 20:20:51 ERROR Executor: Exception in task 3.0 in stage 8.0 (TID 16)\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "\tat java.base/java.io.ByteArrayOutputStream.<init>(ByteArrayOutputStream.java:79)\n",
      "\tat org.apache.iceberg.shaded.org.apache.parquet.hadoop.CodecFactory$HeapBytesCompressor.<init>(CodecFactory.java:158)\n",
      "\tat org.apache.iceberg.shaded.org.apache.parquet.hadoop.CodecFactory.createCompressor(CodecFactory.java:219)\n",
      "\tat org.apache.iceberg.shaded.org.apache.parquet.hadoop.CodecFactory.getCompressor(CodecFactory.java:202)\n",
      "\tat org.apache.iceberg.parquet.ParquetWriter.<init>(ParquetWriter.java:90)\n",
      "\tat org.apache.iceberg.parquet.Parquet$WriteBuilder.build(Parquet.java:352)\n",
      "\tat org.apache.iceberg.parquet.Parquet$DataWriteBuilder.build(Parquet.java:734)\n",
      "\tat org.apache.iceberg.data.BaseFileWriterFactory.newDataWriter(BaseFileWriterFactory.java:131)\n",
      "\tat org.apache.iceberg.io.RollingDataWriter.newWriter(RollingDataWriter.java:52)\n",
      "\tat org.apache.iceberg.io.RollingDataWriter.newWriter(RollingDataWriter.java:32)\n",
      "\tat org.apache.iceberg.io.RollingFileWriter.openCurrentWriter(RollingFileWriter.java:108)\n",
      "\tat org.apache.iceberg.io.RollingDataWriter.<init>(RollingDataWriter.java:47)\n",
      "\tat org.apache.iceberg.io.FanoutDataWriter.newWriter(FanoutDataWriter.java:53)\n",
      "\tat org.apache.iceberg.io.FanoutWriter.writer(FanoutWriter.java:63)\n",
      "\tat org.apache.iceberg.io.FanoutWriter.write(FanoutWriter.java:51)\n",
      "\tat org.apache.iceberg.io.FanoutDataWriter.write(FanoutDataWriter.java:31)\n",
      "\tat org.apache.iceberg.spark.source.SparkWrite$PartitionedDataWriter.write(SparkWrite.java:781)\n",
      "\tat org.apache.iceberg.spark.source.SparkWrite$PartitionedDataWriter.write(SparkWrite.java:751)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.write(WriteToDataSourceV2Exec.scala:493)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.$anonfun$run$1(WriteToDataSourceV2Exec.scala:448)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask$$Lambda$4150/0x0000000101813840.apply(Unknown Source)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1397)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run(WriteToDataSourceV2Exec.scala:486)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run$(WriteToDataSourceV2Exec.scala:425)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.run(WriteToDataSourceV2Exec.scala:491)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.$anonfun$writeWithV2$2(WriteToDataSourceV2Exec.scala:388)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec$$Lambda$3932/0x000000010178d840.apply(Unknown Source)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$Lambda$2863/0x00000001013e6840.apply(Unknown Source)\n",
      "25/01/28 20:20:51 ERROR DataWritingSparkTask: Aborted commit for partition 8 (task 21, attempt 0, stage 8.0)\n",
      "25/01/28 20:20:51 ERROR Executor: Exception in task 8.0 in stage 8.0 (TID 21)\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "25/01/28 20:20:51 ERROR DataWritingSparkTask: Aborted commit for partition 6 (task 19, attempt 0, stage 8.0)\n",
      "25/01/28 20:20:51 ERROR Executor: Exception in task 6.0 in stage 8.0 (TID 19)\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "25/01/28 20:20:51 ERROR SparkUncaughtExceptionHandler: Uncaught exception in thread Thread[Executor task launch worker for task 6.0 in stage 8.0 (TID 19),5,main]\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "25/01/28 20:20:51 ERROR SparkUncaughtExceptionHandler: Uncaught exception in thread Thread[Executor task launch worker for task 7.0 in stage 8.0 (TID 20),5,main]\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "25/01/28 20:20:51 ERROR SparkUncaughtExceptionHandler: Uncaught exception in thread Thread[Executor task launch worker for task 10.0 in stage 8.0 (TID 23),5,main]\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "25/01/28 20:20:51 ERROR SparkUncaughtExceptionHandler: Uncaught exception in thread Thread[Executor task launch worker for task 2.0 in stage 8.0 (TID 15),5,main]\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "25/01/28 20:20:51 ERROR SparkUncaughtExceptionHandler: Uncaught exception in thread Thread[Executor task launch worker for task 8.0 in stage 8.0 (TID 21),5,main]\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "25/01/28 20:20:51 ERROR SparkUncaughtExceptionHandler: Uncaught exception in thread Thread[Executor task launch worker for task 3.0 in stage 8.0 (TID 16),5,main]\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "\tat java.base/java.io.ByteArrayOutputStream.<init>(ByteArrayOutputStream.java:79)\n",
      "\tat org.apache.iceberg.shaded.org.apache.parquet.hadoop.CodecFactory$HeapBytesCompressor.<init>(CodecFactory.java:158)\n",
      "\tat org.apache.iceberg.shaded.org.apache.parquet.hadoop.CodecFactory.createCompressor(CodecFactory.java:219)\n",
      "\tat org.apache.iceberg.shaded.org.apache.parquet.hadoop.CodecFactory.getCompressor(CodecFactory.java:202)\n",
      "\tat org.apache.iceberg.parquet.ParquetWriter.<init>(ParquetWriter.java:90)\n",
      "\tat org.apache.iceberg.parquet.Parquet$WriteBuilder.build(Parquet.java:352)\n",
      "\tat org.apache.iceberg.parquet.Parquet$DataWriteBuilder.build(Parquet.java:734)\n",
      "\tat org.apache.iceberg.data.BaseFileWriterFactory.newDataWriter(BaseFileWriterFactory.java:131)\n",
      "\tat org.apache.iceberg.io.RollingDataWriter.newWriter(RollingDataWriter.java:52)\n",
      "\tat org.apache.iceberg.io.RollingDataWriter.newWriter(RollingDataWriter.java:32)\n",
      "\tat org.apache.iceberg.io.RollingFileWriter.openCurrentWriter(RollingFileWriter.java:108)\n",
      "\tat org.apache.iceberg.io.RollingDataWriter.<init>(RollingDataWriter.java:47)\n",
      "\tat org.apache.iceberg.io.FanoutDataWriter.newWriter(FanoutDataWriter.java:53)\n",
      "\tat org.apache.iceberg.io.FanoutWriter.writer(FanoutWriter.java:63)\n",
      "\tat org.apache.iceberg.io.FanoutWriter.write(FanoutWriter.java:51)\n",
      "\tat org.apache.iceberg.io.FanoutDataWriter.write(FanoutDataWriter.java:31)\n",
      "\tat org.apache.iceberg.spark.source.SparkWrite$PartitionedDataWriter.write(SparkWrite.java:781)\n",
      "\tat org.apache.iceberg.spark.source.SparkWrite$PartitionedDataWriter.write(SparkWrite.java:751)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.write(WriteToDataSourceV2Exec.scala:493)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.$anonfun$run$1(WriteToDataSourceV2Exec.scala:448)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask$$Lambda$4150/0x0000000101813840.apply(Unknown Source)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1397)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run(WriteToDataSourceV2Exec.scala:486)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run$(WriteToDataSourceV2Exec.scala:425)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.run(WriteToDataSourceV2Exec.scala:491)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.$anonfun$writeWithV2$2(WriteToDataSourceV2Exec.scala:388)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec$$Lambda$3932/0x000000010178d840.apply(Unknown Source)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$Lambda$2863/0x00000001013e6840.apply(Unknown Source)\n",
      "25/01/28 20:20:51 ERROR SparkUncaughtExceptionHandler: [Container in shutdown] Uncaught exception in thread Thread[Executor task launch worker for task 5.0 in stage 8.0 (TID 18),5,main]\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "25/01/28 20:20:51 ERROR DataWritingSparkTask: Aborted commit for partition 1 (task 14, attempt 0, stage 8.0)\n",
      "25/01/28 20:20:51 ERROR Executor: Exception in task 1.0 in stage 8.0 (TID 14)\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "25/01/28 20:20:51 ERROR SparkUncaughtExceptionHandler: [Container in shutdown] Uncaught exception in thread Thread[Executor task launch worker for task 1.0 in stage 8.0 (TID 14),5,main]\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "25/01/28 20:20:51 ERROR SparkUncaughtExceptionHandler: Uncaught exception in thread Thread[Executor task launch worker for task 0.0 in stage 8.0 (TID 13),5,main]\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "25/01/28 20:20:51 WARN Utils: Suppressing exception in catch: Task java.util.concurrent.FutureTask@715b7666[Not completed, task = org.apache.iceberg.aws.s3.S3FileIO$$Lambda$5754/0x0000000101666840@4fd3e032] rejected from java.util.concurrent.ThreadPoolExecutor@6c76f33a[Shutting down, pool size = 4, active threads = 0, queued tasks = 0, completed tasks = 9]\n",
      "java.util.concurrent.RejectedExecutionException: Task java.util.concurrent.FutureTask@715b7666[Not completed, task = org.apache.iceberg.aws.s3.S3FileIO$$Lambda$5754/0x0000000101666840@4fd3e032] rejected from java.util.concurrent.ThreadPoolExecutor@6c76f33a[Shutting down, pool size = 4, active threads = 0, queued tasks = 0, completed tasks = 9]\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$AbortPolicy.rejectedExecution(ThreadPoolExecutor.java:2055)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.reject(ThreadPoolExecutor.java:825)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.execute(ThreadPoolExecutor.java:1355)\n",
      "\tat java.base/java.util.concurrent.AbstractExecutorService.submit(AbstractExecutorService.java:140)\n",
      "\tat java.base/java.util.concurrent.Executors$DelegatedExecutorService.submit(Executors.java:719)\n",
      "\tat org.apache.iceberg.aws.s3.S3FileIO.deleteFiles(S3FileIO.java:213)\n",
      "\tat org.apache.iceberg.spark.source.SparkCleanupUtil.bulkDelete(SparkCleanupUtil.java:99)\n",
      "\tat org.apache.iceberg.spark.source.SparkCleanupUtil.deletePaths(SparkCleanupUtil.java:91)\n",
      "\tat org.apache.iceberg.spark.source.SparkCleanupUtil.deleteFiles(SparkCleanupUtil.java:85)\n",
      "\tat org.apache.iceberg.spark.source.SparkCleanupUtil.deleteTaskFiles(SparkCleanupUtil.java:57)\n",
      "\tat org.apache.iceberg.spark.source.SparkWrite$PartitionedDataWriter.abort(SparkWrite.java:799)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.$anonfun$run$6(WriteToDataSourceV2Exec.scala:482)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1408)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run(WriteToDataSourceV2Exec.scala:486)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run$(WriteToDataSourceV2Exec.scala:425)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.run(WriteToDataSourceV2Exec.scala:491)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.$anonfun$writeWithV2$2(WriteToDataSourceV2Exec.scala:388)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "25/01/28 20:20:51 ERROR Executor: Exception in task 9.0 in stage 8.0 (TID 22)\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "\tat java.base/java.io.ByteArrayOutputStream.<init>(ByteArrayOutputStream.java:79)\n",
      "\tat org.apache.iceberg.shaded.org.apache.parquet.hadoop.CodecFactory$HeapBytesCompressor.<init>(CodecFactory.java:158)\n",
      "\tat org.apache.iceberg.shaded.org.apache.parquet.hadoop.CodecFactory.createCompressor(CodecFactory.java:219)\n",
      "\tat org.apache.iceberg.shaded.org.apache.parquet.hadoop.CodecFactory.getCompressor(CodecFactory.java:202)\n",
      "\tat org.apache.iceberg.parquet.ParquetWriter.<init>(ParquetWriter.java:90)\n",
      "\tat org.apache.iceberg.parquet.Parquet$WriteBuilder.build(Parquet.java:352)\n",
      "\tat org.apache.iceberg.parquet.Parquet$DataWriteBuilder.build(Parquet.java:734)\n",
      "\tat org.apache.iceberg.data.BaseFileWriterFactory.newDataWriter(BaseFileWriterFactory.java:131)\n",
      "\tat org.apache.iceberg.io.RollingDataWriter.newWriter(RollingDataWriter.java:52)\n",
      "\tat org.apache.iceberg.io.RollingDataWriter.newWriter(RollingDataWriter.java:32)\n",
      "\tat org.apache.iceberg.io.RollingFileWriter.openCurrentWriter(RollingFileWriter.java:108)\n",
      "\tat org.apache.iceberg.io.RollingDataWriter.<init>(RollingDataWriter.java:47)\n",
      "\tat org.apache.iceberg.io.FanoutDataWriter.newWriter(FanoutDataWriter.java:53)\n",
      "\tat org.apache.iceberg.io.FanoutWriter.writer(FanoutWriter.java:63)\n",
      "\tat org.apache.iceberg.io.FanoutWriter.write(FanoutWriter.java:51)\n",
      "\tat org.apache.iceberg.io.FanoutDataWriter.write(FanoutDataWriter.java:31)\n",
      "\tat org.apache.iceberg.spark.source.SparkWrite$PartitionedDataWriter.write(SparkWrite.java:781)\n",
      "\tat org.apache.iceberg.spark.source.SparkWrite$PartitionedDataWriter.write(SparkWrite.java:751)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.write(WriteToDataSourceV2Exec.scala:493)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.$anonfun$run$1(WriteToDataSourceV2Exec.scala:448)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask$$Lambda$4150/0x0000000101813840.apply(Unknown Source)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1397)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run(WriteToDataSourceV2Exec.scala:486)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run$(WriteToDataSourceV2Exec.scala:425)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.run(WriteToDataSourceV2Exec.scala:491)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.$anonfun$writeWithV2$2(WriteToDataSourceV2Exec.scala:388)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec$$Lambda$3932/0x000000010178d840.apply(Unknown Source)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$Lambda$2863/0x00000001013e6840.apply(Unknown Source)\n",
      "25/01/28 20:20:51 ERROR SparkUncaughtExceptionHandler: [Container in shutdown] Uncaught exception in thread Thread[Executor task launch worker for task 9.0 in stage 8.0 (TID 22),5,main]\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "\tat java.base/java.io.ByteArrayOutputStream.<init>(ByteArrayOutputStream.java:79)\n",
      "\tat org.apache.iceberg.shaded.org.apache.parquet.hadoop.CodecFactory$HeapBytesCompressor.<init>(CodecFactory.java:158)\n",
      "\tat org.apache.iceberg.shaded.org.apache.parquet.hadoop.CodecFactory.createCompressor(CodecFactory.java:219)\n",
      "\tat org.apache.iceberg.shaded.org.apache.parquet.hadoop.CodecFactory.getCompressor(CodecFactory.java:202)\n",
      "\tat org.apache.iceberg.parquet.ParquetWriter.<init>(ParquetWriter.java:90)\n",
      "\tat org.apache.iceberg.parquet.Parquet$WriteBuilder.build(Parquet.java:352)\n",
      "\tat org.apache.iceberg.parquet.Parquet$DataWriteBuilder.build(Parquet.java:734)\n",
      "\tat org.apache.iceberg.data.BaseFileWriterFactory.newDataWriter(BaseFileWriterFactory.java:131)\n",
      "\tat org.apache.iceberg.io.RollingDataWriter.newWriter(RollingDataWriter.java:52)\n",
      "\tat org.apache.iceberg.io.RollingDataWriter.newWriter(RollingDataWriter.java:32)\n",
      "\tat org.apache.iceberg.io.RollingFileWriter.openCurrentWriter(RollingFileWriter.java:108)\n",
      "\tat org.apache.iceberg.io.RollingDataWriter.<init>(RollingDataWriter.java:47)\n",
      "\tat org.apache.iceberg.io.FanoutDataWriter.newWriter(FanoutDataWriter.java:53)\n",
      "\tat org.apache.iceberg.io.FanoutWriter.writer(FanoutWriter.java:63)\n",
      "\tat org.apache.iceberg.io.FanoutWriter.write(FanoutWriter.java:51)\n",
      "\tat org.apache.iceberg.io.FanoutDataWriter.write(FanoutDataWriter.java:31)\n",
      "\tat org.apache.iceberg.spark.source.SparkWrite$PartitionedDataWriter.write(SparkWrite.java:781)\n",
      "\tat org.apache.iceberg.spark.source.SparkWrite$PartitionedDataWriter.write(SparkWrite.java:751)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.write(WriteToDataSourceV2Exec.scala:493)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.$anonfun$run$1(WriteToDataSourceV2Exec.scala:448)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask$$Lambda$4150/0x0000000101813840.apply(Unknown Source)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1397)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run(WriteToDataSourceV2Exec.scala:486)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run$(WriteToDataSourceV2Exec.scala:425)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.run(WriteToDataSourceV2Exec.scala:491)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.$anonfun$writeWithV2$2(WriteToDataSourceV2Exec.scala:388)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec$$Lambda$3932/0x000000010178d840.apply(Unknown Source)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$Lambda$2863/0x00000001013e6840.apply(Unknown Source)\n",
      "25/01/28 20:20:51 WARN Utils: Suppressing exception in catch: Task java.util.concurrent.FutureTask@4271ab4d[Not completed, task = org.apache.iceberg.aws.s3.S3FileIO$$Lambda$5754/0x0000000101666840@560b18c0] rejected from java.util.concurrent.ThreadPoolExecutor@6c76f33a[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 9]\n",
      "java.util.concurrent.RejectedExecutionException: Task java.util.concurrent.FutureTask@4271ab4d[Not completed, task = org.apache.iceberg.aws.s3.S3FileIO$$Lambda$5754/0x0000000101666840@560b18c0] rejected from java.util.concurrent.ThreadPoolExecutor@6c76f33a[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 9]\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$AbortPolicy.rejectedExecution(ThreadPoolExecutor.java:2055)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.reject(ThreadPoolExecutor.java:825)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.execute(ThreadPoolExecutor.java:1355)\n",
      "\tat java.base/java.util.concurrent.AbstractExecutorService.submit(AbstractExecutorService.java:140)\n",
      "\tat java.base/java.util.concurrent.Executors$DelegatedExecutorService.submit(Executors.java:719)\n",
      "\tat org.apache.iceberg.aws.s3.S3FileIO.deleteFiles(S3FileIO.java:213)\n",
      "\tat org.apache.iceberg.spark.source.SparkCleanupUtil.bulkDelete(SparkCleanupUtil.java:99)\n",
      "\tat org.apache.iceberg.spark.source.SparkCleanupUtil.deletePaths(SparkCleanupUtil.java:91)\n",
      "\tat org.apache.iceberg.spark.source.SparkCleanupUtil.deleteFiles(SparkCleanupUtil.java:85)\n",
      "\tat org.apache.iceberg.spark.source.SparkCleanupUtil.deleteTaskFiles(SparkCleanupUtil.java:57)\n",
      "\tat org.apache.iceberg.spark.source.SparkWrite$PartitionedDataWriter.abort(SparkWrite.java:799)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.$anonfun$run$6(WriteToDataSourceV2Exec.scala:482)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1408)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run(WriteToDataSourceV2Exec.scala:486)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run$(WriteToDataSourceV2Exec.scala:425)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.run(WriteToDataSourceV2Exec.scala:491)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.$anonfun$writeWithV2$2(WriteToDataSourceV2Exec.scala:388)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "25/01/28 20:20:51 WARN TaskSetManager: Lost task 5.0 in stage 8.0 (TID 18) (9be1677be336 executor driver): java.lang.OutOfMemoryError: Java heap space\n",
      "\n",
      "25/01/28 20:20:51 ERROR Executor: Exception in task 4.0 in stage 8.0 (TID 17)\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "25/01/28 20:20:51 ERROR SparkUncaughtExceptionHandler: [Container in shutdown] Uncaught exception in thread Thread[Executor task launch worker for task 4.0 in stage 8.0 (TID 17),5,main]\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "25/01/28 20:20:51 ERROR TaskSetManager: Task 5 in stage 8.0 failed 1 times; aborting job\n",
      "25/01/28 20:20:51 ERROR AppendDataExec: Data source write support IcebergBatchWrite(table=demo.bootcamp_hw.match_details_bucketed, format=PARQUET) is aborting.\n",
      "25/01/28 20:20:51 ERROR AppendDataExec: Data source write support IcebergBatchWrite(table=demo.bootcamp_hw.match_details_bucketed, format=PARQUET) aborted.\n",
      "25/01/28 20:20:51 ERROR Utils: Aborting task\n",
      "org.apache.spark.SparkException: Job aborted due to stage failure: Task 5 in stage 8.0 failed 1 times, most recent failure: Lost task 5.0 in stage 8.0 (TID 18) (9be1677be336 executor driver): java.lang.OutOfMemoryError: Java heap space\n",
      "\n",
      "Driver stacktrace:\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\n",
      "\tat scala.Option.foreach(Option.scala:407)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\n",
      "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2398)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.writeWithV2(WriteToDataSourceV2Exec.scala:385)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.writeWithV2$(WriteToDataSourceV2Exec.scala:359)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.AppendDataExec.writeWithV2(WriteToDataSourceV2Exec.scala:225)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2ExistingTableWriteExec.run(WriteToDataSourceV2Exec.scala:337)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2ExistingTableWriteExec.run$(WriteToDataSourceV2Exec.scala:336)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.AppendDataExec.run(WriteToDataSourceV2Exec.scala:225)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result$lzycompute(V2CommandExec.scala:43)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result(V2CommandExec.scala:43)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.executeCollect(V2CommandExec.scala:49)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)\n",
      "\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:142)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2CreateTableAsSelectBaseExec.$anonfun$writeToTable$1(WriteToDataSourceV2Exec.scala:577)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1397)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2CreateTableAsSelectBaseExec.writeToTable(WriteToDataSourceV2Exec.scala:573)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2CreateTableAsSelectBaseExec.writeToTable$(WriteToDataSourceV2Exec.scala:567)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.AtomicReplaceTableAsSelectExec.writeToTable(WriteToDataSourceV2Exec.scala:183)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.AtomicReplaceTableAsSelectExec.run(WriteToDataSourceV2Exec.scala:216)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result$lzycompute(V2CommandExec.scala:43)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result(V2CommandExec.scala:43)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.executeCollect(V2CommandExec.scala:49)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)\n",
      "\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:142)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:859)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.saveAsTable(DataFrameWriter.scala:634)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.saveAsTable(DataFrameWriter.scala:564)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: java.lang.OutOfMemoryError: Java heap space\n",
      "ERROR:root:Exception while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/spark/python/pyspark/errors/exceptions/captured.py\", line 179, in deco\n",
      "    return f(*a, **kw)\n",
      "  File \"/opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/protocol.py\", line 326, in get_return_value\n",
      "    raise Py4JJavaError(\n",
      "py4j.protocol.Py4JJavaError: <exception str() failed>\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py\", line 516, in send_command\n",
      "    raise Py4JNetworkError(\"Answer from Java side is empty\")\n",
      "py4j.protocol.Py4JNetworkError: Answer from Java side is empty\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "  File \"/opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py\", line 539, in send_command\n",
      "    raise Py4JNetworkError(\n",
      "py4j.protocol.Py4JNetworkError: Error while sending or receiving\n"
     ]
    },
    {
     "ename": "Py4JError",
     "evalue": "py4j does not exist in the JVM",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "File \u001b[0;32m/opt/spark/python/pyspark/errors/exceptions/captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    178\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 179\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    180\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31m<class 'str'>\u001b[0m: (<class 'ConnectionRefusedError'>, ConnectionRefusedError(111, 'Connection refused'))",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mPy4JError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m num_buckets \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m16\u001b[39m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Create bucketed tables\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m \u001b[43mmatch_details\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43miceberg\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m                            \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbucketBy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum_buckets\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmatch_id\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m                            \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpartitionBy\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmatch_id\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m                            \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43moverwrite\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m                            \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msaveAsTable\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbootcamp_hw.match_details_bucketed\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m matches\u001b[38;5;241m.\u001b[39mwrite\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124miceberg\u001b[39m\u001b[38;5;124m\"\u001b[39m) \\\n\u001b[1;32m     12\u001b[0m                             \u001b[38;5;241m.\u001b[39mbucketBy(num_buckets, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmatch_id\u001b[39m\u001b[38;5;124m\"\u001b[39m) \\\n\u001b[1;32m     13\u001b[0m                             \u001b[38;5;241m.\u001b[39mpartitionBy(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmatch_id\u001b[39m\u001b[38;5;124m\"\u001b[39m) \\\n\u001b[1;32m     14\u001b[0m                             \u001b[38;5;241m.\u001b[39mmode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moverwrite\u001b[39m\u001b[38;5;124m\"\u001b[39m) \\\n\u001b[1;32m     15\u001b[0m                             \u001b[38;5;241m.\u001b[39msaveAsTable(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbootcamp_hw.matches_bucketed\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     17\u001b[0m medals_matches_players\u001b[38;5;241m.\u001b[39mwrite\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124miceberg\u001b[39m\u001b[38;5;124m\"\u001b[39m) \\\n\u001b[1;32m     18\u001b[0m                             \u001b[38;5;241m.\u001b[39mbucketBy(num_buckets, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmatch_id\u001b[39m\u001b[38;5;124m\"\u001b[39m) \\\n\u001b[1;32m     19\u001b[0m                             \u001b[38;5;241m.\u001b[39mpartitionBy(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmatch_id\u001b[39m\u001b[38;5;124m\"\u001b[39m) \\\n\u001b[1;32m     20\u001b[0m                             \u001b[38;5;241m.\u001b[39mmode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moverwrite\u001b[39m\u001b[38;5;124m\"\u001b[39m) \\\n\u001b[1;32m     21\u001b[0m                             \u001b[38;5;241m.\u001b[39msaveAsTable(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbootcamp_hw.medals_matches_players_bucketed\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/spark/python/pyspark/sql/readwriter.py:1586\u001b[0m, in \u001b[0;36mDataFrameWriter.saveAsTable\u001b[0;34m(self, name, format, mode, partitionBy, **options)\u001b[0m\n\u001b[1;32m   1584\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mformat\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1585\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mformat\u001b[39m)\n\u001b[0;32m-> 1586\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jwrite\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msaveAsTable\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/opt/spark/python/pyspark/errors/exceptions/captured.py:181\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    179\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;241m*\u001b[39ma, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\n\u001b[1;32m    180\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m--> 181\u001b[0m     converted \u001b[38;5;241m=\u001b[39m \u001b[43mconvert_exception\u001b[49m\u001b[43m(\u001b[49m\u001b[43me\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjava_exception\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    182\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    183\u001b[0m         \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    184\u001b[0m         \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[1;32m    185\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/spark/python/pyspark/errors/exceptions/captured.py:159\u001b[0m, in \u001b[0;36mconvert_exception\u001b[0;34m(e)\u001b[0m\n\u001b[1;32m    156\u001b[0m c: Py4JJavaError \u001b[38;5;241m=\u001b[39m e\u001b[38;5;241m.\u001b[39mgetCause()\n\u001b[1;32m    157\u001b[0m stacktrace: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m jvm\u001b[38;5;241m.\u001b[39morg\u001b[38;5;241m.\u001b[39mapache\u001b[38;5;241m.\u001b[39mspark\u001b[38;5;241m.\u001b[39mutil\u001b[38;5;241m.\u001b[39mUtils\u001b[38;5;241m.\u001b[39mexceptionString(e)\n\u001b[1;32m    158\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m c \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m (\n\u001b[0;32m--> 159\u001b[0m     \u001b[43mis_instance_of\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgw\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43morg.apache.spark.api.python.PythonException\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    160\u001b[0m     \u001b[38;5;66;03m# To make sure this only catches Python UDFs.\u001b[39;00m\n\u001b[1;32m    161\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\n\u001b[1;32m    162\u001b[0m         \u001b[38;5;28mmap\u001b[39m(\n\u001b[1;32m    163\u001b[0m             \u001b[38;5;28;01mlambda\u001b[39;00m v: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124morg.apache.spark.sql.execution.python\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m v\u001b[38;5;241m.\u001b[39mtoString(), c\u001b[38;5;241m.\u001b[39mgetStackTrace()\n\u001b[1;32m    164\u001b[0m         )\n\u001b[1;32m    165\u001b[0m     )\n\u001b[1;32m    166\u001b[0m ):\n\u001b[1;32m    167\u001b[0m     msg \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    168\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m  An exception was thrown from the Python worker. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    169\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease see the stack trace below.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m c\u001b[38;5;241m.\u001b[39mgetMessage()\n\u001b[1;32m    170\u001b[0m     )\n\u001b[1;32m    171\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m PythonException(msg, stacktrace)\n",
      "File \u001b[0;32m/opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:464\u001b[0m, in \u001b[0;36mis_instance_of\u001b[0;34m(gateway, java_object, java_class)\u001b[0m\n\u001b[1;32m    460\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    461\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    462\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mjava_class must be a string, a JavaClass, or a JavaObject\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 464\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgateway\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjvm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpy4j\u001b[49m\u001b[38;5;241m.\u001b[39mreflection\u001b[38;5;241m.\u001b[39mTypeUtil\u001b[38;5;241m.\u001b[39misInstanceOf(\n\u001b[1;32m    465\u001b[0m     param, java_object)\n",
      "File \u001b[0;32m/opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1725\u001b[0m, in \u001b[0;36mJVMView.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1722\u001b[0m _, error_message \u001b[38;5;241m=\u001b[39m get_error_message(answer)\n\u001b[1;32m   1723\u001b[0m message \u001b[38;5;241m=\u001b[39m compute_exception_message(\n\u001b[1;32m   1724\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m does not exist in the JVM\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(name), error_message)\n\u001b[0;32m-> 1725\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(message)\n",
      "\u001b[0;31mPy4JError\u001b[0m: py4j does not exist in the JVM"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Exception while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py\", line 516, in send_command\n",
      "    raise Py4JNetworkError(\"Answer from Java side is empty\")\n",
      "py4j.protocol.Py4JNetworkError: Answer from Java side is empty\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "  File \"/opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py\", line 539, in send_command\n",
      "    raise Py4JNetworkError(\n",
      "py4j.protocol.Py4JNetworkError: Error while sending or receiving\n"
     ]
    }
   ],
   "source": [
    "# Declare bucket number\n",
    "num_buckets = 16\n",
    "\n",
    "# Create bucketed tables\n",
    "match_details.write.format(\"iceberg\") \\\n",
    "                            .bucketBy(num_buckets, \"match_id\") \\\n",
    "                            .partitionBy(\"match_id\") \\\n",
    "                            .mode(\"overwrite\") \\\n",
    "                            .saveAsTable(\"bootcamp_hw.match_details_bucketed\")\n",
    "\n",
    "matches.write.format(\"iceberg\") \\\n",
    "                            .bucketBy(num_buckets, \"match_id\") \\\n",
    "                            .partitionBy(\"match_id\") \\\n",
    "                            .mode(\"overwrite\") \\\n",
    "                            .saveAsTable(\"bootcamp_hw.matches_bucketed\")\n",
    "\n",
    "medals_matches_players.write.format(\"iceberg\") \\\n",
    "                            .bucketBy(num_buckets, \"match_id\") \\\n",
    "                            .partitionBy(\"match_id\") \\\n",
    "                            .mode(\"overwrite\") \\\n",
    "                            .saveAsTable(\"bootcamp_hw.medals_matches_players_bucketed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d61e4cc2-54e9-4716-bfd3-ce84bfe592f6",
   "metadata": {},
   "outputs": [
    {
     "ename": "ConnectionRefusedError",
     "evalue": "[Errno 111] Connection refused",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mConnectionRefusedError\u001b[0m                    Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m num_buckets \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m16\u001b[39m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Create bucketed tables\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m \u001b[43mmatch_details\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwriteTo\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbootcamp_hw.match_details_bucketed\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \\\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;241m.\u001b[39moption(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwrite.format.default\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparquet\u001b[39m\u001b[38;5;124m\"\u001b[39m) \\\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;241m.\u001b[39moption(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbucketby\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmatch_id\u001b[39m\u001b[38;5;124m\"\u001b[39m) \\\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;241m.\u001b[39moption(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum-buckets\u001b[39m\u001b[38;5;124m\"\u001b[39m, num_buckets) \\\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;241m.\u001b[39mcreateOrReplace()\n\u001b[1;32m     11\u001b[0m matches\u001b[38;5;241m.\u001b[39mwriteTo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbootcamp_hw.matches_bucketed\u001b[39m\u001b[38;5;124m\"\u001b[39m) \\\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;241m.\u001b[39moption(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwrite.format.default\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparquet\u001b[39m\u001b[38;5;124m\"\u001b[39m) \\\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;241m.\u001b[39moption(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbucketby\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmatch_id\u001b[39m\u001b[38;5;124m\"\u001b[39m) \\\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;241m.\u001b[39moption(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum-buckets\u001b[39m\u001b[38;5;124m\"\u001b[39m, num_buckets) \\\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;241m.\u001b[39mcreateOrReplace()\n\u001b[1;32m     17\u001b[0m medals_matches_players\u001b[38;5;241m.\u001b[39mwriteTo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbootcamp_hw.medals_matches_players_bucketed\u001b[39m\u001b[38;5;124m\"\u001b[39m) \\\n\u001b[1;32m     18\u001b[0m     \u001b[38;5;241m.\u001b[39moption(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwrite.format.default\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparquet\u001b[39m\u001b[38;5;124m\"\u001b[39m) \\\n\u001b[1;32m     19\u001b[0m     \u001b[38;5;241m.\u001b[39moption(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbucketby\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmatch_id\u001b[39m\u001b[38;5;124m\"\u001b[39m) \\\n\u001b[1;32m     20\u001b[0m     \u001b[38;5;241m.\u001b[39moption(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum-buckets\u001b[39m\u001b[38;5;124m\"\u001b[39m, num_buckets) \\\n\u001b[1;32m     21\u001b[0m     \u001b[38;5;241m.\u001b[39mcreateOrReplace()\n",
      "File \u001b[0;32m/opt/spark/python/pyspark/sql/dataframe.py:5717\u001b[0m, in \u001b[0;36mDataFrame.writeTo\u001b[0;34m(self, table)\u001b[0m\n\u001b[1;32m   5685\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwriteTo\u001b[39m(\u001b[38;5;28mself\u001b[39m, table: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrameWriterV2:\n\u001b[1;32m   5686\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   5687\u001b[0m \u001b[38;5;124;03m    Create a write configuration builder for v2 sources.\u001b[39;00m\n\u001b[1;32m   5688\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   5715\u001b[0m \u001b[38;5;124;03m    ... ).partitionedBy(\"col\").createOrReplace()\u001b[39;00m\n\u001b[1;32m   5716\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 5717\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mDataFrameWriterV2\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtable\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/spark/python/pyspark/sql/readwriter.py:2001\u001b[0m, in \u001b[0;36mDataFrameWriterV2.__init__\u001b[0;34m(self, df, table)\u001b[0m\n\u001b[1;32m   1999\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_df \u001b[38;5;241m=\u001b[39m df\n\u001b[1;32m   2000\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_spark \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39msparkSession\n\u001b[0;32m-> 2001\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jwriter \u001b[38;5;241m=\u001b[39m \u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwriteTo\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtable\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1314\u001b[0m args_command, temp_args \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_args(\u001b[38;5;241m*\u001b[39margs)\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m-> 1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m get_return_value(\n\u001b[1;32m   1323\u001b[0m     answer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_id, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname)\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n",
      "File \u001b[0;32m/opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1036\u001b[0m, in \u001b[0;36mGatewayClient.send_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m   1015\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msend_command\u001b[39m(\u001b[38;5;28mself\u001b[39m, command, retry\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, binary\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m   1016\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Sends a command to the JVM. This method is not intended to be\u001b[39;00m\n\u001b[1;32m   1017\u001b[0m \u001b[38;5;124;03m       called directly by Py4J users. It is usually called by\u001b[39;00m\n\u001b[1;32m   1018\u001b[0m \u001b[38;5;124;03m       :class:`JavaMember` instances.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1034\u001b[0m \u001b[38;5;124;03m     if `binary` is `True`.\u001b[39;00m\n\u001b[1;32m   1035\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1036\u001b[0m     connection \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_connection\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1037\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1038\u001b[0m         response \u001b[38;5;241m=\u001b[39m connection\u001b[38;5;241m.\u001b[39msend_command(command)\n",
      "File \u001b[0;32m/opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:284\u001b[0m, in \u001b[0;36mJavaClient._get_connection\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    281\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m    283\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m connection \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m connection\u001b[38;5;241m.\u001b[39msocket \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 284\u001b[0m     connection \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_new_connection\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    285\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m connection\n",
      "File \u001b[0;32m/opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:291\u001b[0m, in \u001b[0;36mJavaClient._create_new_connection\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    287\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_create_new_connection\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    288\u001b[0m     connection \u001b[38;5;241m=\u001b[39m ClientServerConnection(\n\u001b[1;32m    289\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjava_parameters, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpython_parameters,\n\u001b[1;32m    290\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_property, \u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m--> 291\u001b[0m     \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect_to_java_server\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    292\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mset_thread_connection(connection)\n\u001b[1;32m    293\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m connection\n",
      "File \u001b[0;32m/opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:438\u001b[0m, in \u001b[0;36mClientServerConnection.connect_to_java_server\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    435\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mssl_context:\n\u001b[1;32m    436\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msocket \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mssl_context\u001b[38;5;241m.\u001b[39mwrap_socket(\n\u001b[1;32m    437\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msocket, server_hostname\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjava_address)\n\u001b[0;32m--> 438\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msocket\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjava_address\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjava_port\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    439\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstream \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msocket\u001b[38;5;241m.\u001b[39mmakefile(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    440\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_connected \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mConnectionRefusedError\u001b[0m: [Errno 111] Connection refused"
     ]
    }
   ],
   "source": [
    "# Declare bucket number\n",
    "num_buckets = 16\n",
    "\n",
    "# Create bucketed tables\n",
    "match_details.writeTo(\"bootcamp_hw.match_details_bucketed\") \\\n",
    "    .option(\"write.format.default\", \"parquet\") \\\n",
    "    .option(\"bucketby\", \"match_id\") \\\n",
    "    .option(\"num-buckets\", num_buckets) \\\n",
    "    .createOrReplace()\n",
    "\n",
    "matches.writeTo(\"bootcamp_hw.matches_bucketed\") \\\n",
    "    .option(\"write.format.default\", \"parquet\") \\\n",
    "    .option(\"bucketby\", \"match_id\") \\\n",
    "    .option(\"num-buckets\", num_buckets) \\\n",
    "    .createOrReplace()\n",
    "\n",
    "medals_matches_players.writeTo(\"bootcamp_hw.medals_matches_players_bucketed\") \\\n",
    "    .option(\"write.format.default\", \"parquet\") \\\n",
    "    .option(\"bucketby\", \"match_id\") \\\n",
    "    .option(\"num-buckets\", num_buckets) \\\n",
    "    .createOrReplace()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4bbae1b7-6a6f-4167-a1d5-82090970f7b0",
   "metadata": {},
   "outputs": [
    {
     "ename": "ConnectionRefusedError",
     "evalue": "[Errno 111] Connection refused",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mConnectionRefusedError\u001b[0m                    Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mset\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mspark.sql.shuffle.partitions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_buckets\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m spark\u001b[38;5;241m.\u001b[39mconf\u001b[38;5;241m.\u001b[39mset(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mspark.sql.autoBroadcastJoinThreshold\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m      3\u001b[0m spark\u001b[38;5;241m.\u001b[39mconf\u001b[38;5;241m.\u001b[39mset(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mspark.sql.adaptive.enabled\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfalse\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/spark/python/pyspark/sql/conf.py:43\u001b[0m, in \u001b[0;36mRuntimeConfig.set\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;129m@since\u001b[39m(\u001b[38;5;241m2.0\u001b[39m)\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mset\u001b[39m(\u001b[38;5;28mself\u001b[39m, key: \u001b[38;5;28mstr\u001b[39m, value: Union[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mint\u001b[39m, \u001b[38;5;28mbool\u001b[39m]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     42\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Sets the given Spark runtime configuration property.\"\"\"\u001b[39;00m\n\u001b[0;32m---> 43\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jconf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1314\u001b[0m args_command, temp_args \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_args(\u001b[38;5;241m*\u001b[39margs)\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m-> 1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m get_return_value(\n\u001b[1;32m   1323\u001b[0m     answer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_id, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname)\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n",
      "File \u001b[0;32m/opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1036\u001b[0m, in \u001b[0;36mGatewayClient.send_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m   1015\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msend_command\u001b[39m(\u001b[38;5;28mself\u001b[39m, command, retry\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, binary\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m   1016\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Sends a command to the JVM. This method is not intended to be\u001b[39;00m\n\u001b[1;32m   1017\u001b[0m \u001b[38;5;124;03m       called directly by Py4J users. It is usually called by\u001b[39;00m\n\u001b[1;32m   1018\u001b[0m \u001b[38;5;124;03m       :class:`JavaMember` instances.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1034\u001b[0m \u001b[38;5;124;03m     if `binary` is `True`.\u001b[39;00m\n\u001b[1;32m   1035\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1036\u001b[0m     connection \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_connection\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1037\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1038\u001b[0m         response \u001b[38;5;241m=\u001b[39m connection\u001b[38;5;241m.\u001b[39msend_command(command)\n",
      "File \u001b[0;32m/opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:284\u001b[0m, in \u001b[0;36mJavaClient._get_connection\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    281\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m    283\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m connection \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m connection\u001b[38;5;241m.\u001b[39msocket \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 284\u001b[0m     connection \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_new_connection\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    285\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m connection\n",
      "File \u001b[0;32m/opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:291\u001b[0m, in \u001b[0;36mJavaClient._create_new_connection\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    287\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_create_new_connection\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    288\u001b[0m     connection \u001b[38;5;241m=\u001b[39m ClientServerConnection(\n\u001b[1;32m    289\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjava_parameters, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpython_parameters,\n\u001b[1;32m    290\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_property, \u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m--> 291\u001b[0m     \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect_to_java_server\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    292\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mset_thread_connection(connection)\n\u001b[1;32m    293\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m connection\n",
      "File \u001b[0;32m/opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:438\u001b[0m, in \u001b[0;36mClientServerConnection.connect_to_java_server\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    435\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mssl_context:\n\u001b[1;32m    436\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msocket \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mssl_context\u001b[38;5;241m.\u001b[39mwrap_socket(\n\u001b[1;32m    437\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msocket, server_hostname\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjava_address)\n\u001b[0;32m--> 438\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msocket\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjava_address\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjava_port\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    439\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstream \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msocket\u001b[38;5;241m.\u001b[39mmakefile(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    440\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_connected \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mConnectionRefusedError\u001b[0m: [Errno 111] Connection refused"
     ]
    }
   ],
   "source": [
    "spark.conf.set(\"spark.sql.shuffle.partitions\", num_buckets)\n",
    "spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", -1)\n",
    "spark.conf.set(\"spark.sql.adaptive.enabled\", \"false\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5996199f-5954-441f-bbac-78f9c62bbe7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# spark.sql(\"\"\"\n",
    "#   SELECT * \n",
    "#   FROM bootcamp_hw.matches_bucketed mb\n",
    "#   JOIN bootcamp_hw.match_details_bucketed mdb\n",
    "#     ON mb.match_id = mdb.match_id\n",
    "# \"\"\").explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1999fa09-1929-4a0f-9bf3-431db351bd7d",
   "metadata": {},
   "outputs": [
    {
     "ename": "ConnectionRefusedError",
     "evalue": "[Errno 111] Connection refused",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mConnectionRefusedError\u001b[0m                    Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m match_details_bucketed \u001b[38;5;241m=\u001b[39m \u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtable\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbootcamp_hw.match_details_bucketed\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m matches_bucketed \u001b[38;5;241m=\u001b[39m spark\u001b[38;5;241m.\u001b[39mtable(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbootcamp_hw.matches_bucketed\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      3\u001b[0m medals_matches_players_bucketed \u001b[38;5;241m=\u001b[39m spark\u001b[38;5;241m.\u001b[39mtable(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbootcamp_hw.medals_matches_players_bucketed\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/spark/python/pyspark/sql/session.py:1667\u001b[0m, in \u001b[0;36mSparkSession.table\u001b[0;34m(self, tableName)\u001b[0m\n\u001b[1;32m   1636\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtable\u001b[39m(\u001b[38;5;28mself\u001b[39m, tableName: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame:\n\u001b[1;32m   1637\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Returns the specified table as a :class:`DataFrame`.\u001b[39;00m\n\u001b[1;32m   1638\u001b[0m \n\u001b[1;32m   1639\u001b[0m \u001b[38;5;124;03m    .. versionadded:: 2.0.0\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1665\u001b[0m \u001b[38;5;124;03m    +---+\u001b[39;00m\n\u001b[1;32m   1666\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1667\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m DataFrame(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jsparkSession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtable\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtableName\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;28mself\u001b[39m)\n",
      "File \u001b[0;32m/opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1314\u001b[0m args_command, temp_args \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_args(\u001b[38;5;241m*\u001b[39margs)\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m-> 1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m get_return_value(\n\u001b[1;32m   1323\u001b[0m     answer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_id, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname)\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n",
      "File \u001b[0;32m/opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1036\u001b[0m, in \u001b[0;36mGatewayClient.send_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m   1015\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msend_command\u001b[39m(\u001b[38;5;28mself\u001b[39m, command, retry\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, binary\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m   1016\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Sends a command to the JVM. This method is not intended to be\u001b[39;00m\n\u001b[1;32m   1017\u001b[0m \u001b[38;5;124;03m       called directly by Py4J users. It is usually called by\u001b[39;00m\n\u001b[1;32m   1018\u001b[0m \u001b[38;5;124;03m       :class:`JavaMember` instances.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1034\u001b[0m \u001b[38;5;124;03m     if `binary` is `True`.\u001b[39;00m\n\u001b[1;32m   1035\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1036\u001b[0m     connection \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_connection\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1037\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1038\u001b[0m         response \u001b[38;5;241m=\u001b[39m connection\u001b[38;5;241m.\u001b[39msend_command(command)\n",
      "File \u001b[0;32m/opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:284\u001b[0m, in \u001b[0;36mJavaClient._get_connection\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    281\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m    283\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m connection \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m connection\u001b[38;5;241m.\u001b[39msocket \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 284\u001b[0m     connection \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_new_connection\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    285\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m connection\n",
      "File \u001b[0;32m/opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:291\u001b[0m, in \u001b[0;36mJavaClient._create_new_connection\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    287\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_create_new_connection\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    288\u001b[0m     connection \u001b[38;5;241m=\u001b[39m ClientServerConnection(\n\u001b[1;32m    289\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjava_parameters, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpython_parameters,\n\u001b[1;32m    290\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_property, \u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m--> 291\u001b[0m     \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect_to_java_server\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    292\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mset_thread_connection(connection)\n\u001b[1;32m    293\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m connection\n",
      "File \u001b[0;32m/opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:438\u001b[0m, in \u001b[0;36mClientServerConnection.connect_to_java_server\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    435\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mssl_context:\n\u001b[1;32m    436\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msocket \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mssl_context\u001b[38;5;241m.\u001b[39mwrap_socket(\n\u001b[1;32m    437\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msocket, server_hostname\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjava_address)\n\u001b[0;32m--> 438\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msocket\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjava_address\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjava_port\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    439\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstream \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msocket\u001b[38;5;241m.\u001b[39mmakefile(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    440\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_connected \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mConnectionRefusedError\u001b[0m: [Errno 111] Connection refused"
     ]
    }
   ],
   "source": [
    "match_details_bucketed = spark.table(\"bootcamp_hw.match_details_bucketed\")\n",
    "matches_bucketed = spark.table(\"bootcamp_hw.matches_bucketed\")\n",
    "medals_matches_players_bucketed = spark.table(\"bootcamp_hw.medals_matches_players_bucketed\")\n",
    "\n",
    "\n",
    "\n",
    "matches_bucketed \\\n",
    "    .join(\n",
    "        match_details_bucketed,\n",
    "        on = \"match_id\",\n",
    "        how = \"inner\"\n",
    "    ) \\\n",
    "    .explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8751df2-1001-47dc-9c08-410f09c6a701",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "484107bb-91f4-44c3-85ae-a550cccf95bb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e36c15c-ddcb-4972-8143-4b0ac176c56e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e34356d-6ea3-4a8d-b6e3-5eeac3a23a9e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "351b5c17-888f-4c52-a8f3-0baadad54428",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79477e8e-333a-42b0-acd1-7f812520a1f6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b6e6391-4598-4b02-988f-d8b1cd67fd37",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b844fd82-5755-41b4-aa20-fe40c7774ace",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cf2b3bc-cdf8-4b5d-852a-b95b93549a2a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfd47245-f7db-468a-b1f8-145ccff8ebcb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "972ffa7a-741b-48af-9216-279b9d4b20e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "val matches_bucketed = spark.read.option(\"header\", \"true\")\n",
    "                        .option(\"inferSchema\", \"true\")\n",
    "                        .csv(\"/home/iceberg/data/matches.csv\")\n",
    "                        .withColumn(\"event_ts\", to_timestamp(col(\"completion_date\")))\n",
    "                        .withColumn(\"event_date\", expr(\"DATE_TRUNC('DAY', completion_date)\"))\n",
    "\n",
    "val match_details_bucketed = spark.read.option(\"header\", \"true\")\n",
    "                        .option(\"inferSchema\", \"true\")\n",
    "                        .csv(\"/home/iceberg/data/match_details.csv\")\n",
    "\n",
    "val medals_matches_players_bucketed = spark.read.option(\"header\", \"true\")\n",
    "                        .option(\"inferSchema\", \"true\")\n",
    "                        .csv(\"/home/iceberg/data/medals_matches_players.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e1aeec9-fe95-43cf-87a8-c4a44ddee253",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"\"\"DROP TABLE IF EXISTS bootcamp_hw.matches_bucketed\"\"\")\n",
    "\n",
    "val matches_bucketed_DDL = \"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS bootcamp_hw.matches_bucketed (\n",
    "      match_id STRING,\n",
    "      mapid STRING,\n",
    "      is_team_game BOOLEAN,\n",
    "      playlist_id STRING,\n",
    "      game_variant_id STRING,\n",
    "      is_match_over BOOLEAN,\n",
    "      completion_date TIMESTAMP,\n",
    "      match_duration STRING,\n",
    "      game_mode STRING,\n",
    "      map_variant_id STRING,\n",
    "      event_date DATE\n",
    "    )\n",
    "    USING iceberg\n",
    "    PARTITIONED BY (completion_date, bucket(16, match_id));\n",
    "\"\"\"\n",
    "spark.sql(matches_bucketed_DDL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23c2a963-5970-4007-8243-338e1a15d08a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f7bf868-72ee-43c7-9da2-9c73dbd7b503",
   "metadata": {},
   "outputs": [],
   "source": [
    "matches_bucketed.select(\n",
    "  $\"match_id\",\n",
    "  $\"mapid\",\n",
    "  $\"is_team_game\",\n",
    "  $\"playlist_id\",\n",
    "  $\"game_variant_id\",\n",
    "  $\"is_match_over\",\n",
    "  $\"completion_date\",\n",
    "  $\"match_duration\",\n",
    "  $\"game_mode\",\n",
    "  $\"map_variant_id\",\n",
    "  $\"event_date\",\n",
    ")\n",
    "    .write.mode(\"append\")\n",
    "    .partitionBy(\"completion_date\")\n",
    "    .bucketBy(16, \"match_id\")\n",
    "    .saveAsTable(\"bootcamp_hw.matches_bucketed\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f634b97-97e3-4d47-a1ef-2330d03ec423",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"\"\"DROP TABLE IF EXISTS bootcamp_hw.match_details_bucketed\"\"\")\n",
    "\n",
    "val match_details_bucketed_DDL = \"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS bootcamp_hw.match_details_bucketed (\n",
    "      match_id STRING,\n",
    "      player_gamertag STRING,\n",
    "      player_total_kills INTEGER,\n",
    "      player_total_headshots INTEGER,\n",
    "      player_total_deaths INTEGER,\n",
    "      did_win INTEGER,\n",
    "      team_id STRING\n",
    "    )\n",
    "    USING iceberg\n",
    "    PARTITIONED BY (bucket(16, match_id));\n",
    "\"\"\"\n",
    "spark.sql(match_details_bucketed_DDL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26c9d4cf-2a73-4a3b-beb0-19fe83de1d27",
   "metadata": {},
   "outputs": [],
   "source": [
    "match_details_bucketed.select(\n",
    "  $\"match_id\",\n",
    "  $\"player_gamertag\",\n",
    "  $\"player_total_kills\",\n",
    "  $\"player_total_headshots\",\n",
    "  $\"player_total_deaths\",\n",
    "  $\"did_win\",\n",
    "  $\"team_id\",\n",
    ")\n",
    "    .write.mode(\"append\")\n",
    "    .bucketBy(16, \"match_id\")\n",
    "    .saveAsTable(\"bootcamp_hw.match_details_bucketed\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "273166ad-e69e-412c-8dfe-e92e00d45241",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"\"\"DROP TABLE IF EXISTS bootcamp_hw.medals_matches_players_bucketed\"\"\")\n",
    "\n",
    "val medals_matches_players_bucketed_DDL = \"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS bootcamp_hw.medals_matches_players_bucketed(\n",
    "      match_id STRING,\n",
    "      player_gamertag STRING,\n",
    "      medal_id LONG,\n",
    "      count INTEGER\n",
    "    )\n",
    "    USING iceberg\n",
    "    PARTITIONED BY (bucket(16, match_id));\n",
    "\"\"\"\n",
    "spark.sql(medals_matches_players_bucketed_DDL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f421fb1-2b89-49fd-905c-a8eed8f559e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "medals_matches_players_bucketed.select(\n",
    "  $\"match_id\",\n",
    "  $\"player_gamertag\",\n",
    "  $\"medal_id\",\n",
    "  $\"count\",\n",
    ")\n",
    "    .write.mode(\"append\")\n",
    "    .bucketBy(16, \"match_id\")\n",
    "    .saveAsTable(\"bootcamp_hw.medals_matches_players_bucketed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59d065aa-3f06-4eb0-b37a-28163d8dbbb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"\"\"DROP TABLE IF EXISTS bootcamp_hw.matches_full_details_bucketed\"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0451c347-d451-4eea-a876-f77ad4ef21e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", -1)\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "  SELECT \n",
    "    mb.match_id\n",
    "  , mb.mapid\n",
    "  , mb.is_team_game\n",
    "  , mb.playlist_id\n",
    "  , mb.game_variant_id\n",
    "  , mb.is_match_over\n",
    "  , mb.completion_date\n",
    "  , mb.match_duration\n",
    "  , mb.game_mode\n",
    "  , mb.map_variant_id\n",
    "  , mb.event_date\n",
    "  , mdb.player_gamertag\n",
    "  , mdb.player_total_kills\n",
    "  , mdb.player_total_headshots\n",
    "  , mdb.player_total_deaths\n",
    "  , mdb.did_win\n",
    "  , mdb.team_id\n",
    "  , mmpb.medal_id\n",
    "  , mmpb.count\n",
    "  FROM bootcamp_hw.matches_bucketed mb\n",
    "  LEFT JOIN bootcamp_hw.match_details_bucketed mdb\n",
    "    ON mb.match_id = mdb.match_id\n",
    "  LEFT JOIN bootcamp_hw.medals_matches_players_bucketed mmpb\n",
    "    ON mdb.match_id = mmpb.match_id AND mdb.player_gamertag = mmpb.player_gamertag\n",
    "\"\"\"\n",
    ").explain()\n",
    "      "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0837e42e-6693-4d60-be8e-e28c23ea0ba7",
   "metadata": {},
   "source": [
    "Which player averages the most kills per game?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "741a7d67-9355-44c6-a4f1-73b2b0f72ab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"\"\"\n",
    "  WITH match_summary AS (\n",
    "    SELECT \n",
    "      player_gamertag\n",
    "    , COUNT(match_id) AS num_matches\n",
    "    , COUNT(DISTINCT match_id) AS num_distinct_matches\n",
    "    , SUM(player_total_kills) AS total_kills\n",
    "    FROM matches_full_details_bucketed\n",
    "    WHERE 1=1\n",
    "    GROUP BY 1\n",
    "  )\n",
    "\n",
    "  , avg_kills_top_10 AS (\n",
    "    SELECT \n",
    "      player_gamertag\n",
    "    , total_kills / COALESCE(num_matches, 1) AS avg_kills\n",
    "    FROM match_summary\n",
    "    WHERE 1=1\n",
    "    ORDER BY 2 DESC\n",
    "    LIMIT 10\n",
    "  )\n",
    "\n",
    "  SELECT *\n",
    "  FROM avg_kills_top_10\n",
    "\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44fe1393-66d4-44ef-b8eb-e3fb087a2e8e",
   "metadata": {},
   "source": [
    "Which playlist gets played the most?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf7b310f-7a87-40c6-a420-cedb3617687d",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"\"\"\n",
    "  WITH match_summary AS (\n",
    "    SELECT \n",
    "      playlist_id\n",
    "    , COUNT(DISTINCT match_id) AS num_distinct_matches\n",
    "    FROM matches_full_details_bucketed\n",
    "    WHERE 1=1\n",
    "    GROUP BY 1\n",
    "  )\n",
    "\n",
    "  SELECT *\n",
    "  FROM match_summary\n",
    "  WHERE 1=1\n",
    "  ORDER BY 2 DESC\n",
    "  LIMIT 10\n",
    "\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83e96f4c-0167-43b1-9b1b-50e5dd7efc28",
   "metadata": {},
   "source": [
    "Which map gets played the most?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17ebb94d-3697-46ad-b966-c5d38fc3d81e",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"\"\"\n",
    "  WITH match_summary AS (\n",
    "    SELECT \n",
    "      mapid AS map_id\n",
    "    , COUNT(DISTINCT match_id) AS num_distinct_matches\n",
    "    FROM matches_full_details_bucketed\n",
    "    WHERE 1=1\n",
    "    GROUP BY 1\n",
    "  )\n",
    "\n",
    "  SELECT *\n",
    "  FROM match_summary\n",
    "  WHERE 1=1\n",
    "  ORDER BY 2 DESC\n",
    "  LIMIT 10\n",
    "\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea13d0e5-a803-41ad-b3ad-13509c185a79",
   "metadata": {},
   "source": [
    "Which map do players get the most Killing Spree medals on?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a3af90f-b5af-4f28-aef5-f49734967ff2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
